{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ohsumed collection\n",
    "it includes medical abstracts from the MeSH categories of \n",
    "the year 1991. In [Joachims, 1997] were used the first 20,000 documents divided\n",
    " in 10,000 for training and 10,000 for testing. The specific task was to categorize \n",
    "the 23 cardiovascular diseases categories. After selecting the such category \n",
    "subset, the unique abstract number becomes 13,929 (6,286 for training and \n",
    "7,643 for testing). As current computers can easily manage larger number of \n",
    "documents we make available all 34,389 cardiovascular diseases abstracts \n",
    "out of 50,216 medical abstracts contained in the year 1991.\n",
    "\n",
    "\"Bacterial Infections and Mycoses                      C01\n",
    "Virus Diseases                                        C02\n",
    "Parasitic Diseases                                    C03\n",
    "Neoplasms                                             C04\n",
    "Musculoskeletal Diseases                              C05\n",
    "Digestive System Diseases                             C06\n",
    "Stomatognathic Diseases                               C07\n",
    "Respiratory Tract Diseases                            C08\n",
    "Otorhinolaryngologic Diseases                         C09\n",
    "Nervous System Diseases                               C10\n",
    "Eye Diseases                                          C11\n",
    "Urologic and Male Genital Diseases                    C12\n",
    "Female Genital Diseases and Pregnancy Complications   C13\n",
    "Cardiovascular Diseases                               C14\n",
    "Hemic and Lymphatic Diseases                          C15\n",
    "Neonatal Diseases and Abnormalities                   C16\n",
    "Skin and Connective Tissue Diseases                   C17\n",
    "Nutritional and Metabolic Diseases                    C18\n",
    "Endocrine Diseases                                    C19\n",
    "Immunologic Diseases                                  C20\n",
    "Disorders of Environmental Origin                     C21\n",
    "Animal Diseases                                       C22\n",
    "Pathological Conditions, Signs and Symptoms           C23\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ohsumed-file-category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_list = sorted([name for name in os.listdir(\".\") if os.path.isdir(name)])\n",
    "category_list = ['C01', 'C02', 'C03', 'C04', 'C05', 'C06', 'C07', 'C08', 'C09', 'C10', 'C11', 'C12', 'C13', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21', 'C22', 'C23']\n",
    "df = pd.DataFrame([], columns = [\"file_name\"] + category_list)\n",
    "\n",
    "category_len = len(category_list)\n",
    "category_index_dict = dict(zip(category_list, list(range(category_len))))\n",
    "\n",
    "for category in category_list:\n",
    "    file_names = os.listdir('./data-original/' + category)\n",
    "    for file in file_names:\n",
    "        df_file_name = df.loc[(df['file_name'] == file)]\n",
    "        if df_file_name.empty:\n",
    "            data = [0] * category_len\n",
    "            data[category_index_dict[category]] = 1\n",
    "            data = [file] + data\n",
    "            df = df.append(pd.DataFrame([data], columns = [\"file_name\"] + category_list), ignore_index=True)\n",
    "        else:\n",
    "            # chack for duplicates\n",
    "            if len(df_file_name) > 1:\n",
    "                raise Exception(\"Duplicates have found!\")\n",
    "            else:\n",
    "                file_index = df.index[df['file_name'] == file][0]\n",
    "                df.at[file_index, category] = 1\n",
    "\n",
    "# df.to_csv('ohsumed-file-category.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ohsumed-file-abstract-category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_list = [name for name in os.listdir(os.listdir(\"./data-original/.\")) if os.path.isdir(os.listdir(\"./data-original/\"+name))]\n",
    "df = pd.DataFrame([], columns = [\"file_name\", \"title_abstract\"] + category_list)\n",
    "\n",
    "category_len = len(category_list)\n",
    "category_index_dict = dict(zip(category_list, list(range(category_len))))\n",
    "\n",
    "for category in category_list:\n",
    "    file_names = os.listdir('./data-original/'+category)\n",
    "    for file in file_names:\n",
    "        df_file_name = df.loc[(df['file_name'] == file)]\n",
    "        if df_file_name.empty:\n",
    "            data = [0] * category_len\n",
    "            data[category_index_dict[category]] = 1\n",
    "            if file[0] == '.':\n",
    "                continue\n",
    "            abstract = open('./data-original/'+category+'/'+file, 'r').read()\n",
    "            data = [file, abstract] + data\n",
    "            df = df.append(pd.DataFrame([data], columns = [\"file_name\", \"title_abstract\"] + category_list), ignore_index=True)\n",
    "        else:\n",
    "            # chack for duplicates\n",
    "            if len(df_file_name) > 1:\n",
    "                raise Exception(\"Duplicates have found!\")\n",
    "            else:\n",
    "                file_index = df.index[df['file_name'] == file][0]\n",
    "                df.at[file_index, category] = 1\n",
    "                \n",
    "# df.to_csv('ohsumed-file-abstract-category.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and creating N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "from gensim.models import Phrases\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data-processed/ohsumed-file-abstract-category.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Celaning raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cleaned = []\n",
    "\n",
    "# Replace all numbers with special strings\n",
    "regx = re.compile(r\"\\b[\\d.]+\\b\")\n",
    "porter = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    text = row['title_abstract'].replace('Copyright', '').split('Â©', 1)[0]\n",
    "    # with stemming\n",
    "#     text = [porter.stem(word.strip()) for word in nltk.word_tokenize(text.lower()) if (word not in string.punctuation) and (word not in stopwords.words(\"english\"))]\n",
    "    \n",
    "#     # without stemming\n",
    "#     text = [word.strip() for word in nltk.word_tokenize(text.lower()) if (word not in string.punctuation) and (word not in stopwords.words(\"english\"))]\n",
    "    \n",
    "    # with lemmatizer\n",
    "    text = [wordnet_lemmatizer.lemmatize(word.strip()) for word in nltk.word_tokenize(text.lower()) if (word not in string.punctuation) and (word not in stopwords.words(\"english\"))]\n",
    "         \n",
    "    text_cleaned.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Findining Phrases (ie bi-grams)\n",
    "# # train bi-grams\n",
    "# bigram = Phrases()\n",
    "# bigram.add_vocab(text_cleaned)\n",
    "\n",
    "# # create phrases\n",
    "# text_cleaned_phrases = []\n",
    "# for text_ in text_cleaned:\n",
    "#     text_cleaned_phrases.append(bigram[text_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate N-grams for abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text_cleaned, n):\n",
    "    text_tokenized = []\n",
    "    for tokens in text_cleaned:\n",
    "        s = ''\n",
    "        for ngram in nltk.ngrams(tokens, n):\n",
    "            s = s + '_'.join(str(i) for i in ngram) + ' '\n",
    "        text_tokenized.append(s)\n",
    "    return text_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1 # n-grams\n",
    "criteria = ['C04', 'C12'] # categories\n",
    "\n",
    "text_tokenized = generate_ngrams(text_cleaned, n)\n",
    "df_ngrams = df[['file_name'] + criteria]\n",
    "df_ngrams['Y'] = df_ngrams.apply(lambda r: 1 if r[criteria[0]]+r[criteria[1]] == 2 else 0, axis=1)\n",
    "df_ngrams['tokens'] = pd.Series(text_tokenized, index=df_ngrams.index)\n",
    "df_ngrams.to_csv('ohsumed_{}_{}_{}grams.csv'.format(criteria[0], criteria[1], n), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
