{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories  = ['apparel', 'automotive', 'baby', 'beauty', 'books', 'camera_&_photo', 'cell_phones_&_service',\n",
    "               'computer_&_video_games', 'dvd', 'electronics', 'gourmet_food', 'grocery', 'health_&_personal_care',\n",
    "               'jewelry_&_watches', 'kitchen_&_housewares', 'magazines', 'music', 'musical_instruments', 'office_products',\n",
    "               'outdoor_living', 'software', 'sports_&_outdoors', 'tools_&_hardware', 'toys_&_games', 'video']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cleaning raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start process the data from scratch, you need to set is_from_raw_amaxon_dataset = True \n",
    "# download and unzip unprocessed.tar.gz  from https://www.cs.jhu.edu/~mdredze/datasets/sentiment/\n",
    "# put the unzipped folders into data/data-raw/\n",
    "\n",
    "is_from_raw_amaxon_dataset = False\n",
    "\n",
    "if is_from_raw_amaxon_dataset:\n",
    "    # empty dataframe for saving counts on categories\n",
    "    columns=['category', 'all_review_num', 'neg_review_num', 'neg_review_selectivity']\n",
    "    df_counts = pd.DataFrame([], columns=columns)\n",
    "    \n",
    "    for category in categories:\n",
    "\n",
    "        with open('data/data-raw/{}/all.review'.format(category), 'r', encoding = \"ISO-8859-1\") as myfile:\n",
    "            data=myfile.read().replace('\\n', '')\n",
    "\n",
    "        reviews = re.findall(\"<review_text>(.*?)</review_text>\", data)\n",
    "        ratings = re.findall(\"<rating>(.*?)</rating>\", data)\n",
    "\n",
    "        df = pd.DataFrame(list(zip(reviews, ratings)), columns=['text', 'star']).drop_duplicates(subset=['text'])\n",
    "        # Keep reviews with 1, 2, 5 stars\n",
    "        df = df.loc[df['star'].isin(['1.0', '5.0'])]\n",
    "        df['is_negative'] = df['star'].map({'1.0': 1, '5.0': 0})\n",
    "\n",
    "        print('{} | rev_num: {}, neg_rev_num: {}, neg_rev_selectivity: {}'.\n",
    "              format(category, df.shape[0], sum(df['is_negative']), sum(df['is_negative'])/ df.shape[0]))\n",
    "\n",
    "        # add current category counts to df2\n",
    "        df_temp = pd.DataFrame([[category, df.shape[0], sum(df['is_negative']),  sum(df['is_negative'])/ df.shape[0]]], columns=columns)\n",
    "        df_counts = df_counts.append(df_temp)\n",
    "\n",
    "#         # save clean data to csv\n",
    "#         df.to_csv('data/clean/{}.csv'.format(category), index=False)\n",
    "        \n",
    "#     df_counts.to_csv('data/category_counts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Books selectivity = 0.61 <br/>\n",
    "Negative review selectivity = 0.10 <br/>\n",
    "Proportion of IN-scope reviews = 0.05 <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Making a dataset of size N from the cleaned dataset  (see \"1. Cleaning raw data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "#   do stratified sampling of reviews, so keeping the same proportion of positive \n",
    "#   and negatives reviews as in the original dataset\n",
    "def do_stratified_sample(df_category, reviews_to_sample):\n",
    "    indexs, y = df_category.index.values, df_category['is_negative'].values\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=reviews_to_sample, random_state=seed)\n",
    "    for _, index_ in sss.split(indexs, y):\n",
    "        df_category_index = indexs[index_]\n",
    "    \n",
    "    return df_category_index\n",
    "\n",
    "N = 100000  # size of new dataset\n",
    "\n",
    "df_counts = pd.read_csv('data/category_counts.csv')\n",
    "df_counts['reviews_to_sample'] = N * df_counts['all_review_num'] / sum(df_counts['all_review_num'].values)\n",
    "df_counts['reviews_to_sample'] = df_counts['reviews_to_sample'].apply(lambda x: round(x))\n",
    "\n",
    "# round reviews_to_sample to be equal to N in total\n",
    "if sum(df_counts['reviews_to_sample'].values) < N:\n",
    "    add_num = N - sum(df_counts['reviews_to_sample'].values)\n",
    "    book_ind = df_counts.index[df_counts['category'] == 'books'][0]\n",
    "    df_counts.loc[book_ind, 'reviews_to_sample'] = df_counts.loc[book_ind, 'reviews_to_sample'] + add_num\n",
    "\n",
    "# empty dataframe for future dataset in size of N\n",
    "columns_dataset_n = ['text', 'is_negative', 'is_book', 'Y', 'category']\n",
    "df_dataset_n = pd.DataFrame([], columns=columns_dataset_n)\n",
    "\n",
    "for category in categories:\n",
    "    df_category = pd.read_csv('data/clean/{}.csv'.format(category))[['text', 'is_negative']]\n",
    "    reviews_to_sample = df_counts[df_counts['category'] == category]['reviews_to_sample'].values[0]\n",
    "    \n",
    "#     check if reviews_to_sample == 1, than take the first review as StratifiedShuffleSplit will raise ValueError\n",
    "    if reviews_to_sample > 1:\n",
    "        df_category_index = do_stratified_sample(df_category, reviews_to_sample)\n",
    "    elif reviews_to_sample == 1:\n",
    "        df_category_index = [1]\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "#   add additional columns 'is_book', 'Y', and 'category'\n",
    "    df_to_add = df_category.loc[df_category_index]\n",
    "    if category == 'books':\n",
    "        df_to_add['is_book'] = 1\n",
    "        df_to_add['Y'] = df_to_add.apply(lambda row: 1 if row['is_negative']+row['is_book'] == 2 else 0, axis=1)\n",
    "    else:\n",
    "        df_to_add['is_book'] = 0\n",
    "        df_to_add['Y'] = 0\n",
    "    df_to_add['category'] = category\n",
    "    \n",
    "#   append current cutegory dataframe to df_dataset_n\n",
    "    df_dataset_n = df_dataset_n.append(df_to_add)\n",
    "\n",
    "# save dataframe to csv file\n",
    "df_dataset_n.to_csv('data/clean-sized/{}_reviews.csv'.format(N), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stemming, Lemmatising, finding phrases the AMAZON reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/clean-sized/100000_reviews.csv')\n",
    "text_cleaned = []\n",
    "\n",
    "# Replace all numbers with special strings\n",
    "regx = re.compile(r\"\\b[\\d.]+\\b\")\n",
    "porter = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    text = row['text']\n",
    "#     with stemming\n",
    "    text = [porter.stem(word.strip()) for word in nltk.word_tokenize(text.lower()) if (word not in string.punctuation) and (word not in stopwords.words(\"english\"))]\n",
    "    \n",
    "#     # without stemming\n",
    "#     text = [word.strip() for word in nltk.word_tokenize(text.lower()) if (word not in string.punctuation) and (word not in stopwords.words(\"english\"))]\n",
    "    \n",
    "#     # with lemmatizer\n",
    "#     text = [wordnet_lemmatizer.lemmatize(word.strip()) for word in nltk.word_tokenize(text.lower()) if (word not in string.punctuation) and (word not in stopwords.words(\"english\"))]\n",
    "         \n",
    "    text_cleaned.append(text)\n",
    "    \n",
    "# Findining Phrases (ie bi-grams)\n",
    "# train bi-grams\n",
    "bigram = Phrases()\n",
    "bigram.add_vocab(text_cleaned)\n",
    "\n",
    "# create phrases\n",
    "text_cleaned_phrases = []\n",
    "for text_ in text_cleaned:\n",
    "    text_cleaned_phrases.append(bigram[text_])\n",
    "\n",
    "text_cleaned_phrases_joined = [' '.join(text) for text in text_cleaned_phrases]\n",
    "df['text'] = pd.Series(text_cleaned_phrases_joined, index=df.index)\n",
    "\n",
    "df.to_csv('100000_reviews_stemmed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
